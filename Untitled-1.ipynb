{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 255, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Get_gradient(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Get_gradient, self).__init__()\n",
    "        kernel_v = [[0, -1, 0], \n",
    "                    [0, 0, 0], \n",
    "                    [0, 1, 0]]\n",
    "        kernel_h = [[0, 0, 0], \n",
    "                    [-1, 0, 1], \n",
    "                    [0, 0, 0]]\n",
    "        kernel_h = torch.FloatTensor(kernel_h).unsqueeze(0).unsqueeze(0)\n",
    "        kernel_v = torch.FloatTensor(kernel_v).unsqueeze(0).unsqueeze(0)\n",
    "        self.weight_h = nn.Parameter(data = kernel_h, requires_grad = False).cpu()\n",
    "        self.weight_v = nn.Parameter(data = kernel_v, requires_grad = False).cpu()\n",
    "\n",
    "    def forward(self, x, input_nc=3):\n",
    "        if input_nc==3:\n",
    "            x0 = x[:, 0]\n",
    "            x1 = x[:, 1]\n",
    "            x2 = x[:, 2]\n",
    "            x0_v = F.conv2d(x0.unsqueeze(1), self.weight_v, padding=1)\n",
    "            x0_h = F.conv2d(x0.unsqueeze(1), self.weight_h, padding=1)\n",
    "\n",
    "            x1_v = F.conv2d(x1.unsqueeze(1), self.weight_v, padding=1)\n",
    "            x1_h = F.conv2d(x1.unsqueeze(1), self.weight_h, padding=1)\n",
    "\n",
    "            x2_v = F.conv2d(x2.unsqueeze(1), self.weight_v, padding=1)\n",
    "            x2_h = F.conv2d(x2.unsqueeze(1), self.weight_h, padding=1)\n",
    "\n",
    "            x0 = torch.sqrt(torch.pow(x0_v, 2) + torch.pow(x0_h, 2) + 1e-6)\n",
    "            x1 = torch.sqrt(torch.pow(x1_v, 2) + torch.pow(x1_h, 2) + 1e-6)\n",
    "            x2 = torch.sqrt(torch.pow(x2_v, 2) + torch.pow(x2_h, 2) + 1e-6)\n",
    "\n",
    "            x = torch.cat([x0, x1, x2], dim=1)\n",
    "        elif input_nc==1:\n",
    "            x = x[:,0]\n",
    "            x_v = F.conv2d(x.unsqueeze(1), self.weight_v, padding=1)\n",
    "            x_h = F.conv2d(x.unsqueeze(1), self.weight_h, padding=1)\n",
    "            x = torch.sqrt(torch.pow(x_v, 2) + torch.pow(x_h, 2) + 1e-6)\n",
    "        return x\n",
    "\n",
    "input = torch.zeros(1,255,255,1)\n",
    "model = Get_gradient()\n",
    "model(input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch import ViT\n",
    "import torch\n",
    "\n",
    "\n",
    "model = ViT(\n",
    "        image_size=256,\n",
    "        patch_size=32,\n",
    "        num_classes=2,\n",
    "        dim=1024,\n",
    "        depth=6,\n",
    "        heads=16,\n",
    "        mlp_dim=2048,\n",
    "        dropout=0.1,\n",
    "        emb_dropout=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "import copy\n",
    "\n",
    "class ContentLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, target,):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # we 'detach' the target content from the tree used\n",
    "        # to dynamically compute the gradient: this is a stated value,\n",
    "        # not a variable. Otherwise the forward method of the criterion\n",
    "        # will throw an error.\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "def gram_matrix(input):\n",
    "    a, b, c, d = input.size()  # a=batch size(=1)\n",
    "    # b=number of feature maps\n",
    "    # (c,d)=dimensions of a f. map (N=c*d)\n",
    "\n",
    "    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n",
    "\n",
    "    G = torch.mm(features, features.t())  # compute the gram product\n",
    "\n",
    "    # we 'normalize' the values of the gram matrix\n",
    "    # by dividing by the number of element in each feature maps.\n",
    "    return G.div(a * b * c * d)\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input\n",
    "\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # normalize img\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
    "                               style_img,\n",
    "                               style_layers=['conv_2', 'conv_4', 'conv_7', 'conv_10']):\n",
    "    # At runtime, CNN is a pretrained VGG19 CNN network.\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "\n",
    "    normalization = Normalization(normalization_mean, normalization_std)\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    i = 0  # increment every time we see a conv\n",
    "    for layer in cnn.children():\n",
    "        # The first layer simply puts names to things and replaces ReLU inplace\n",
    "        # (which is optimized) with ReLU reallocated. This is a small optimization\n",
    "        # being removed, and hence a small performance penalty, necessitated by\n",
    "        # ContentLoss and StyleLoss not working well when inplace=True.\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'bn_{}'.format(i)\n",
    "        else:\n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "        # add_module is a setter that is pretty much a setattr equivalent, used for\n",
    "        # registering the layer with PyTorch.\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # Trim off the layers after the last content and style losses\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "\n",
    "    model = model[:(i + 1)]\n",
    "\n",
    "    return model, style_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.zeros(1,3,256,256)\n",
    "cnn = models.vgg19(pretrained=True).features.eval()\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "temp_model, style_losses = get_style_model_and_losses(\n",
    "    cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "    img\n",
    ")\n",
    "\n",
    "input = torch.ones(1,3,256,256)\n",
    "input.data.clamp_(0,1)\n",
    "style_score = []\n",
    "x = input\n",
    "# for i in range(len(temp_model)):\n",
    "#     x = temp_model[i](x)\n",
    "#     if i in [4]:\n",
    "#         print(temp_model[i])\n",
    "#         print(temp_model[i](x).loss)\n",
    "#         style_score.append(x)\n",
    "temp_model(input)\n",
    "style_score = 0\n",
    "for sl in style_losses:\n",
    "    style_score += sl.loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_model"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd7cab7c88bdb20641d6dc9b305732e3e3550557ef647e67d9a16fc4dfca0e13"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
