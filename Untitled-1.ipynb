{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03412953820946817 0.034129276280342055\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import numpy as np\n",
    "\n",
    "scale = 2\n",
    "hr = torch.ones(1,1,255,255)\n",
    "sr = torch.ones(1,1,255,255)*255\n",
    "rgb_range = 255\n",
    "\n",
    "\n",
    "diff = (sr - hr) / rgb_range\n",
    "shave = scale\n",
    "if diff.size(1) > 1:\n",
    "    gray_coeffs = [65.738, 129.057, 25.064]\n",
    "    convert = diff.new_tensor(gray_coeffs).view(1, 3, 1, 1) / 256\n",
    "    diff = diff.mul(convert).sum(dim=1)\n",
    "\n",
    "valid = diff[..., shave:-shave, shave:-shave]\n",
    "# valid = diff[...,:,:]\n",
    "mse = valid.pow(2).mean()\n",
    "psnr_value = -10 * math.log10(mse)\n",
    "\n",
    "psnr_value_2 = psnr(hr.numpy(),sr.numpy(),data_range=255)\n",
    "print(psnr_value,psnr_value_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "from src.model.swinir import SwinIR\n",
    "# from src.model.edsr import EDSR\n",
    "\n",
    "class EMA():\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                self.backup[name] = param.data\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "net_g = SwinIR()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "net_g.eval()\n",
    "def ema(model, model_ema, decay=0.999):\n",
    "    model_params = dict(model.named_parameters())\n",
    "    model_ema_params = dict(model_ema.named_parameters())\n",
    "    for k in model_params:\n",
    "        model_ema_params[k].data.mul_(decay).add_(model_params[k].data, alpha=1 - decay)\n",
    "    return model_ema\n",
    "\n",
    "for p in net_g.parameters():\n",
    "    p.requires_grad = True\n",
    "model_ema = copy.deepcopy(net_g)\n",
    "for p in model_ema.parameters():\n",
    "    p.requires_grad = False\n",
    "model_ema = ema(net_g,model_ema)\n",
    "for p in model_ema.parameters():\n",
    "    print(p)\n",
    "# model_ema.eval()\n",
    "# input = torch.zeros(1,3,25,25)\n",
    "# out1 = net_g(input)[0,0,0,0]\n",
    "# out2 = model_ema(input)[0,0,0,0]\n",
    "# print(id(model_ema))\n",
    "# model_ema = ema(net_g,model_ema)\n",
    "# out3 = model_ema(input)[0,0,0,0]\n",
    "# print(out1,out2,out3)\n",
    "# print(id(model_ema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "def forward(output,hr):\n",
    "    sr = output[0]\n",
    "    sr_ema = output[1]\n",
    "    residual_ema = torch.sum(torch.abs(hr - sr_ema), 1, keepdim=True)\n",
    "    residual_SR = torch.sum(torch.abs(hr - sr), 1, keepdim=True)\n",
    "\n",
    "    patch_level_weight = torch.var(residual_SR.clone(), dim=(-1, -2, -3), keepdim=True) ** (1/5)\n",
    "    pixel_level_weight = get_local_weights(residual_SR.clone())\n",
    "    overall_weight = patch_level_weight * pixel_level_weight\n",
    "\n",
    "    overall_weight[residual_SR < residual_ema] = 0\n",
    "    l1 = torch.nn.L1Loss()\n",
    "    out = l1(overall_weight*sr,overall_weight*hr)\n",
    "    return out\n",
    "\n",
    "def get_local_weights(residual, ksize=7):\n",
    "    pad = (ksize - 1) // 2\n",
    "    residual_pad = F.pad(residual, pad=[pad, pad, pad, pad], mode='reflect')\n",
    "    unfolded_residual = residual_pad.unfold(2, ksize, 1).unfold(3, ksize, 1)\n",
    "    pixel_level_weight = torch.var(unfolded_residual, dim=(-1, -2), unbiased=True, keepdim=True).squeeze(-1).squeeze(-1)\n",
    "    return pixel_level_weight\n",
    "\n",
    "hr = torch.randn(8,1,64,64)\n",
    "sr_ema = torch.randn(8,1,64,64)\n",
    "sr = torch.randn(8,1,64,64)*255\n",
    "torch.max(get_local_weights(sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# b,c,h,w -> b,c,h,w\n",
    "class Get_gradient(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Get_gradient, self).__init__()\n",
    "        kernel_v = [[0, -1, 0], \n",
    "                    [0, 0, 0], \n",
    "                    [0, 1, 0]]\n",
    "        kernel_h = [[0, 0, 0], \n",
    "                    [-1, 0, 1], \n",
    "                    [0, 0, 0]]\n",
    "        kernel_h = torch.FloatTensor(kernel_h).unsqueeze(0).unsqueeze(0)\n",
    "        kernel_v = torch.FloatTensor(kernel_v).unsqueeze(0).unsqueeze(0)\n",
    "        self.weight_h = nn.Parameter(data = kernel_h, requires_grad = False).cpu()\n",
    "        self.weight_v = nn.Parameter(data = kernel_v, requires_grad = False).cpu()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_list = []\n",
    "        for i in range(x.shape[1]):\n",
    "            x_i = x[:, i]\n",
    "            x_i_v = F.conv2d(x_i.unsqueeze(1), self.weight_v, padding=1)\n",
    "            x_i_h = F.conv2d(x_i.unsqueeze(1), self.weight_h, padding=1)\n",
    "            x_i = torch.sqrt(torch.pow(x_i_v, 2) + torch.pow(x_i_h, 2) + 1e-6)\n",
    "            x_list.append(x_i)\n",
    "\n",
    "        x = torch.cat(x_list, dim = 1)\n",
    "        return x\n",
    "\n",
    "input = torch.zeros(5,3,200,255)\n",
    "model = Get_gradient()\n",
    "model(input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch import ViT\n",
    "import torch\n",
    "\n",
    "\n",
    "model = ViT(\n",
    "        image_size=256,\n",
    "        patch_size=32,\n",
    "        num_classes=2,\n",
    "        dim=1024,\n",
    "        depth=6,\n",
    "        heads=16,\n",
    "        mlp_dim=2048,\n",
    "        dropout=0.1,\n",
    "        emb_dropout=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "import copy\n",
    "\n",
    "class ContentLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, target,):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # we 'detach' the target content from the tree used\n",
    "        # to dynamically compute the gradient: this is a stated value,\n",
    "        # not a variable. Otherwise the forward method of the criterion\n",
    "        # will throw an error.\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "def gram_matrix(input):\n",
    "    a, b, c, d = input.size()  # a=batch size(=1)\n",
    "    # b=number of feature maps\n",
    "    # (c,d)=dimensions of a f. map (N=c*d)\n",
    "\n",
    "    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n",
    "\n",
    "    G = torch.mm(features, features.t())  # compute the gram product\n",
    "\n",
    "    # we 'normalize' the values of the gram matrix\n",
    "    # by dividing by the number of element in each feature maps.\n",
    "    return G.div(a * b * c * d)\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input\n",
    "\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # normalize img\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
    "                               style_img,\n",
    "                               style_layers=['conv_2', 'conv_4', 'conv_7', 'conv_10']):\n",
    "    # At runtime, CNN is a pretrained VGG19 CNN network.\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "\n",
    "    normalization = Normalization(normalization_mean, normalization_std)\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    i = 0  # increment every time we see a conv\n",
    "    for layer in cnn.children():\n",
    "        # The first layer simply puts names to things and replaces ReLU inplace\n",
    "        # (which is optimized) with ReLU reallocated. This is a small optimization\n",
    "        # being removed, and hence a small performance penalty, necessitated by\n",
    "        # ContentLoss and StyleLoss not working well when inplace=True.\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'bn_{}'.format(i)\n",
    "        else:\n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "        # add_module is a setter that is pretty much a setattr equivalent, used for\n",
    "        # registering the layer with PyTorch.\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # Trim off the layers after the last content and style losses\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "\n",
    "    model = model[:(i + 1)]\n",
    "\n",
    "    return model, style_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.zeros(1,3,256,256)\n",
    "cnn = models.vgg19(pretrained=True).features.eval()\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "temp_model, style_losses = get_style_model_and_losses(\n",
    "    cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "    img\n",
    ")\n",
    "\n",
    "input = torch.ones(1,3,256,256)\n",
    "input.data.clamp_(0,1)\n",
    "style_score = []\n",
    "x = input\n",
    "# for i in range(len(temp_model)):\n",
    "#     x = temp_model[i](x)\n",
    "#     if i in [4]:\n",
    "#         print(temp_model[i])\n",
    "#         print(temp_model[i](x).loss)\n",
    "#         style_score.append(x)\n",
    "temp_model(input)\n",
    "style_score = 0\n",
    "for sl in style_losses:\n",
    "    style_score += sl.loss\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd7cab7c88bdb20641d6dc9b305732e3e3550557ef647e67d9a16fc4dfca0e13"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
